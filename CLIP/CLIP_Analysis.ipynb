{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Performance Analysis (using OpenCLIP)\n",
    "\n",
    "This notebook organizes the code to replicate key findings of CLIP using the `open-clip-torch` library. We will perform three main analyses:\n",
    "\n",
    "1.  **Zero-Shot Evaluation:** Testing the model's out-of-the-box performance using only text prompts (`k=0`).\n",
    "2.  **Full Linear-Probe Evaluation:** Testing the quality of the image features by training a classifier on the *entire* training set (`k=ALL`).\n",
    "3.  **Few-Shot Evaluation:** Testing the data-efficiency of the features by training a classifier on `k=1, 2, 4, 8, 16` samples per class.\n",
    "\n",
    "Finally, we will visualize these results by loading the `.json` files generated by each analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "First, we import all necessary libraries. We have replaced `clip` with `open_clip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import open_clip\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from dataset_helpers import FLOWERS102_CLASS_NAMES\n",
    "# Import all datasets\n",
    "from torchvision.datasets import (CIFAR100, Food101, Flowers102, DTD, EuroSAT)\n",
    "from utils import extract_features, train_classifier, load_results, get_zeroshot_classifier, run_zeroshot_evaluation\n",
    "from dataset_config import ZERO_SHOT_DATASET_CONFIG, LINEAR_PROBE_DATASET_CONFIG\n",
    "# Import Scikit-learn for linear/few-shot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"All libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print(f\"Loading OpenCLIP model: {MODEL_NAME} ({PRETRAINED_TAG})\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m DEVICE = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m DEVICE\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# print(f\"Loading OpenCLIP model: {MODEL_NAME} ({PRETRAINED_TAG})\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not manually load libiomp5md.dll: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# Import torch explicitly before other libs\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ctypes\n",
    "\n",
    "# Force load the OpenMP library if it exists\n",
    "try:\n",
    "    # Adjust this path if your environment location is different, \n",
    "    # but based on your logs, this is where it should be:\n",
    "    dll_path = r\"C:\\Users\\iivs\\Desktop\\FM Mdels Research\\Iimplementation and Analysis\\analysis\\Lib\\site-packages\\torch\\lib\\libiomp5md.dll\"\n",
    "    if os.path.exists(dll_path):\n",
    "        ctypes.CDLL(dll_path)\n",
    "except Exception as e:\n",
    "    print(f\"Could not manually load libiomp5md.dll: {e}\")\n",
    "\n",
    "import torch  # Import torch explicitly before other libs\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Configuration & Helper Data\n",
    "\n",
    "We define our constants and load the OpenCLIP model here. We use the `'ViT-B-32'` architecture with `'openai'` pretrained weights to replicate the original paper's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get current working directory ---\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# --- Global Constants ---\n",
    "MODEL_NAME = \"ViT-B-32\"\n",
    "PRETRAINED_TAG = \"openai\"\n",
    "BATCH_SIZE = 128\n",
    "DATA_ROOT = os.path.join(cwd, \"cache\")\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)  # Ensure cache folder exists\n",
    "\n",
    "LOGISTIC_REGRESSION_C = 0.316  # From the paper's README\n",
    "K_SHOTS = [1, 2, 4, 8, 16]  # For few-shot analysis\n",
    "\n",
    "# --- Result Folder and File Names ---\n",
    "RESULTS_FOLDER = os.path.join(cwd, \"results\")\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)  # Ensure results folder exists\n",
    "\n",
    "ZERO_SHOT_RESULTS_FILE = os.path.join(RESULTS_FOLDER, \"zero_shot_results_lib.json\")\n",
    "LINEAR_PROBE_RESULTS_FILE = os.path.join(RESULTS_FOLDER, \"linear_probe_results_lib.json\")\n",
    "FEW_SHOT_RESULTS_FILE = os.path.join(RESULTS_FOLDER, \"few_shot_results_lib.json\")\n",
    "\n",
    "# --- Load OpenCLIP Model Once ---\n",
    "print(f\"Loading OpenCLIP model: {MODEL_NAME} ({PRETRAINED_TAG})\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Use create_model_and_transforms for OpenCLIP\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED_TAG, device=DEVICE)\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "\n",
    "model.eval()  # Ensure model is in eval mode\n",
    "print(f\"Model loaded and running on {DEVICE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# ANALYSIS 1: Zero-Shot Evaluation\n",
    "\n",
    "This experiment tests CLIP's ability to classify images using only natural language prompts.\n",
    "\n",
    "**Changes for OpenCLIP:** We now use the `tokenizer` we loaded earlier instead of `clip.tokenize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ► Run Analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_analysis_1():\n",
    "    all_results = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"pretrained\": PRETRAINED_TAG,\n",
    "        \"evaluation_time\": time.time(),\n",
    "        \"scores\": {}\n",
    "    }\n",
    "\n",
    "    for dataset_name, config in ZERO_SHOT_DATASET_CONFIG.items():\n",
    "        print(f\"\\n--- Evaluating Dataset: {dataset_name} ---\")\n",
    "\n",
    "        try:\n",
    "            dataset = config[\"loader\"](preprocess)\n",
    "            dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "            \n",
    "            class_names = config[\"class_getter\"](dataset)\n",
    "            if class_names is None:\n",
    "                raise ValueError(\"Class names could not be loaded.\")\n",
    "            \n",
    "            print(f\"Total test samples: {len(dataset)}, Classes: {len(class_names)}\")\n",
    "            \n",
    "            # Pass tokenizer explicitly\n",
    "            text_classifier = get_zeroshot_classifier(model, tokenizer, config[\"templates\"], class_names)\n",
    "            \n",
    "            accuracy, correct, total = run_zeroshot_evaluation(model, text_classifier, dataloader)\n",
    "            \n",
    "            all_results[\"scores\"][dataset_name] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"correct\": correct,\n",
    "                \"total\": total,\n",
    "                \"class_count\": len(class_names)\n",
    "            }\n",
    "            print(f\"Accuracy for {dataset_name}: {accuracy:.3f}%\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED to evaluate {dataset_name} !!!\")\n",
    "            print(f\"Error: {e}\")\n",
    "            all_results[\"scores\"][dataset_name] = f\"FAILED ({e})\"\n",
    "    \n",
    "    with open(ZERO_SHOT_RESULTS_FILE, 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"\\nAll zero-shot results saved to {ZERO_SHOT_RESULTS_FILE}\")\n",
    "\n",
    "# Set to True to run the analysis\n",
    "if True: \n",
    "    main_analysis_1()\n",
    "else:\n",
    "    print(\"Skipping Analysis 1. Set 'if False' to 'if True' to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# ANALYSIS 2: Full Linear-Probe Evaluation\n",
    "\n",
    "This remains largely unchanged as `extract_features` handles the OpenCLIP differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ► Run Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_analysis_2():\n",
    "    all_results = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"pretrained\": PRETRAINED_TAG,\n",
    "        \"evaluation_time\": time.time(),\n",
    "        \"scores\": {}\n",
    "    }\n",
    "\n",
    "    for dataset_name, config in LINEAR_PROBE_DATASET_CONFIG.items():\n",
    "        print(f\"\\n--- Evaluating Dataset: {dataset_name} ---\")\n",
    "\n",
    "        try:\n",
    "            train_dataset = config[\"train\"](preprocess)\n",
    "            test_dataset = config[\"test\"](preprocess)\n",
    "            print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "            print(\"Extracting features for training set...\")\n",
    "            train_features, train_labels = extract_features(train_dataset, model)\n",
    "            print(\"Extracting features for test set...\")\n",
    "            test_features, test_labels = extract_features(test_dataset, model)\n",
    "            \n",
    "            print(f\"Train features shape: {train_features.shape}\")\n",
    "            print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "            classifier = train_classifier(train_features, train_labels, verbose=1)\n",
    "            \n",
    "            accuracy, total = evaluate_classifier(classifier, test_features, test_labels)\n",
    "            \n",
    "            all_results[\"scores\"][dataset_name] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"total_test_samples\": total,\n",
    "                \"train_samples\": len(train_dataset),\n",
    "            }\n",
    "            print(f\"Accuracy for {dataset_name}: {accuracy:.3f}%\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED to evaluate {dataset_name} !!!\")\n",
    "            print(f\"Error: {e}\")\n",
    "            all_results[\"scores\"][dataset_name] = f\"FAILED ({e})\"\n",
    "    \n",
    "    with open(LINEAR_PROBE_RESULTS_FILE, 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"\\nAll linear-probe results saved to {LINEAR_PROBE_RESULTS_FILE}\")\n",
    "\n",
    "# Set to True to run the analysis\n",
    "if True:\n",
    "    main_analysis_2()\n",
    "else:\n",
    "    print(\"Skipping Analysis 2. Set 'if False' to 'if True' to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# ANALYSIS 3: Few-Shot Evaluation\n",
    "\n",
    "This relies on the same `extract_features` and standard scikit-learn, so no changes needed here either beyond what was done in Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Configuration for Analysis 3 ---\n",
    "FEW_SHOT_DATASET_CONFIG = LINEAR_PROBE_DATASET_CONFIG\n",
    "\n",
    "# --- Helper Functions for Analysis 3 ---\n",
    "\n",
    "def get_k_shot_indices(dataset, k):\n",
    "    \"\"\"\n",
    "    Sub-samples a dataset to get exactly k samples per class.\n",
    "    \"\"\"\n",
    "    indices_per_class = {}\n",
    "    \n",
    "    print(f\"Sampling {k}-shot indices...\")\n",
    "    targets = []\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = dataset.targets\n",
    "    elif hasattr(dataset, '_labels'):\n",
    "        targets = dataset._labels\n",
    "    elif hasattr(dataset, '_samples'): # For ImageFolder-like\n",
    "        targets = [s[1] for s in dataset._samples]\n",
    "    else: # Fallback for datasets like DTD\n",
    "        print(\"Dataset has no standard target attribute, iterating...\")\n",
    "        targets = [label for _, label in tqdm(dataset)]\n",
    "\n",
    "    if not targets:\n",
    "        raise ValueError(\"Could not extract labels from dataset.\")\n",
    "\n",
    "    for idx, target in enumerate(targets):\n",
    "        if target not in indices_per_class:\n",
    "            indices_per_class[target] = []\n",
    "        if len(indices_per_class[target]) < k:\n",
    "            indices_per_class[target].append(idx)\n",
    "            \n",
    "    final_indices = [idx for class_indices in indices_per_class.values() for idx in class_indices]\n",
    "    \n",
    "    num_classes = len(indices_per_class)\n",
    "    print(f\"Sampled {len(final_indices)} total indices for {k} shots across {num_classes} classes.\")\n",
    "    return final_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ► Run Analysis 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_analysis_3():\n",
    "    all_results = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"pretrained\": PRETRAINED_TAG,\n",
    "        \"evaluation_time\": time.time(),\n",
    "        \"scores\": {}\n",
    "    }\n",
    "\n",
    "    # --- Pre-extract all features to speed up k-shot loop ---\n",
    "    test_features_cache = {}\n",
    "    test_labels_cache = {}\n",
    "    train_features_cache = {}\n",
    "    train_labels_cache = {}\n",
    "    train_dataset_cache = {} # Store datasets to sample from\n",
    "    \n",
    "    print(\"--- Pre-extracting ALL features (this will take a while) ---\")\n",
    "    for dataset_name, config in FEW_SHOT_DATASET_CONFIG.items():\n",
    "        try:\n",
    "            print(f\"\\nLoading features for {dataset_name}...\")\n",
    "            train_dataset = config[\"train\"](preprocess)\n",
    "            test_dataset = config[\"test\"](preprocess)\n",
    "            \n",
    "            train_features_cache[dataset_name], train_labels_cache[dataset_name] = extract_features(train_dataset, model)\n",
    "            test_features_cache[dataset_name], test_labels_cache[dataset_name] = extract_features(test_dataset, model)\n",
    "            train_dataset_cache[dataset_name] = train_dataset # Save dataset object for sampling\n",
    "            \n",
    "            print(f\"Features for {dataset_name} cached.\")\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED to cache features for {dataset_name}: {e} !!!\")\n",
    "\n",
    "    # --- Main K-Shot Evaluation Loop ---\n",
    "    print(\"\\n--- Starting Few-Shot Evaluation Loop ---\")\n",
    "    \n",
    "    for dataset_name in train_features_cache.keys(): # Iterate only over cached datasets\n",
    "        print(f\"\\n--- Evaluating Dataset: {dataset_name} ---\")\n",
    "        all_results[\"scores\"][dataset_name] = {}\n",
    "        \n",
    "        for k in K_SHOTS:\n",
    "            try:\n",
    "                print(f\"  Running {k}-shot evaluation...\")\n",
    "                k_shot_indices = get_k_shot_indices(train_dataset_cache[dataset_name], k)\n",
    "                \n",
    "                k_shot_features = train_features_cache[dataset_name][k_shot_indices]\n",
    "                k_shot_labels = train_labels_cache[dataset_name][k_shot_indices]\n",
    "                \n",
    "                classifier = train_classifier(k_shot_features, k_shot_labels, verbose=0) # No verbose\n",
    "                \n",
    "                accuracy, _ = evaluate_classifier(classifier, test_features_cache[dataset_name], test_labels_cache[dataset_name])\n",
    "                print(f\"  Accuracy for {k}-shot: {accuracy:.3f}%\")\n",
    "                all_results[\"scores\"][dataset_name][f\"{k}-shot\"] = accuracy\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!!! FAILED {k}-shot for {dataset_name}: {e} !!!\")\n",
    "                all_results[\"scores\"][dataset_name][f\"{k}-shot\"] = f\"FAILED ({e})\"\n",
    "    \n",
    "    with open(FEW_SHOT_RESULTS_FILE, 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"\\nAll few-shot results saved to {FEW_SHOT_RESULTS_FILE}\")\n",
    "\n",
    "# Set to True to run the analysis\n",
    "if True:\n",
    "    main_analysis_3()\n",
    "else:\n",
    "    print(\"Skipping Analysis 3. Set 'if False' to 'if True' to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# VISUALIZATIONS\n",
    "\n",
    "The visualization code remains exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Paper's Reported Scores for ViT-B/32 ---\n",
    "# PAPER_SCORES = {\n",
    "#     \"Zero-Shot\": {\n",
    "#         \"CIFAR100\": 64.2,\n",
    "#         \"Food101\": 83.3,\n",
    "#         \"Flowers102\": 66.6,\n",
    "#         \"DTD\": 42.9,\n",
    "#         \"EuroSAT\": 40.6,\n",
    "#         \"STL10\": 97.1,\n",
    "#         \"FGVCAircraft\": 22.5,\n",
    "#         \"MNIST\": 29.8,\n",
    "#         \"Country211\": 23.2,\n",
    "#     },\n",
    "#     \"Linear-Probe\": {\n",
    "#         \"CIFAR100\": 80.1,\n",
    "#         \"Food101\": 88.2,\n",
    "#         \"Flowers102\": 93.2,\n",
    "#         \"DTD\": 73.4,\n",
    "#         \"EuroSAT\": 97.2,\n",
    "#         \"STL10\": 98.6,\n",
    "#         \"FGVCAircraft\": 62.0,\n",
    "#         \"MNIST\": 98.4,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # --- Plot A: Zero-Shot vs. Linear-Probe Gain ---\n",
    "# def plot_zeroshot_vs_linear(zs_results, lp_results):\n",
    "#     print(\"--- Generating Plot A: Zero-Shot vs. Linear-Probe Gain ---\")\n",
    "#     datasets = sorted(lp_results.keys()) \n",
    "    \n",
    "#     zs_scores = []\n",
    "#     lp_scores = []\n",
    "#     labels = []\n",
    "\n",
    "#     for ds in datasets:\n",
    "#         if ds in zs_results and isinstance(zs_results[ds], dict) and isinstance(lp_results[ds], dict):\n",
    "#             zs_scores.append(zs_results[ds]['accuracy'])\n",
    "#             lp_scores.append(lp_results[ds]['accuracy'])\n",
    "#             labels.append(ds)\n",
    "    \n",
    "#     if not labels:\n",
    "#         print(\"No common valid results to plot for ZeroShot vs Linear.\")\n",
    "#         return\n",
    "\n",
    "#     y = np.arange(len(labels))\n",
    "#     width = 0.35\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(12, 10))\n",
    "#     ax.barh(y - width/2, zs_scores, width, label='Zero-Shot (k=0)', color='blue')\n",
    "#     ax.barh(y + width/2, lp_scores, width, label='Full Linear-Probe (k=ALL)', color='orange')\n",
    "    \n",
    "#     ax.set_yticks(y)\n",
    "#     ax.set_yticklabels(labels)\n",
    "#     ax.set_xlabel('Accuracy (%)')\n",
    "#     ax.set_title('Zero-Shot vs. Full Linear-Probe Performance')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
    "#     ax.set_xlim(0, 100)\n",
    "    \n",
    "#     for i, (zs, lp) in enumerate(zip(zs_scores, lp_scores)):\n",
    "#         ax.text(zs + 1, y[i] - width/2, f'{zs:.1f}', va='center', ha='left', fontsize=8)\n",
    "#         ax.text(lp + 1, y[i] + width/2, f'{lp:.1f}', va='center', ha='left', fontsize=8, color='darkorange')\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"zeroshot_vs_linear_analysis_library.png\", dpi=300)\n",
    "#     print(\"Plot saved to zeroshot_vs_linear_analysis_library.png\")\n",
    "#     plt.show()\n",
    "\n",
    "# # --- Plot B: Replication Analysis (vs. Paper) ---\n",
    "# def plot_replication_analysis(zs_results, lp_results):\n",
    "#     print(\"--- Generating Plot B: Replication Analysis vs. Paper ---\")\n",
    "    \n",
    "#     paper_zs_keys = set(PAPER_SCORES[\"Zero-Shot\"].keys())\n",
    "#     paper_lp_keys = set(PAPER_SCORES[\"Linear-Probe\"].keys())\n",
    "#     local_zs_keys = set(k for k,v in zs_results.items() if isinstance(v, dict))\n",
    "#     local_lp_keys = set(k for k,v in lp_results.items() if isinstance(v, dict))\n",
    "    \n",
    "#     all_datasets = sorted(list((paper_zs_keys | paper_lp_keys | local_zs_keys | local_lp_keys)))\n",
    "    \n",
    "#     local_scores = []\n",
    "#     paper_scores = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for ds in all_datasets:\n",
    "#         if ds in zs_results and isinstance(zs_results[ds], dict) and ds in PAPER_SCORES[\"Zero-Shot\"]:\n",
    "#             local_scores.append(zs_results[ds]['accuracy'])\n",
    "#             paper_scores.append(PAPER_SCORES[\"Zero-Shot\"][ds])\n",
    "#             labels.append(f\"{ds} (Zero-Shot)\")\n",
    "            \n",
    "#     for ds in all_datasets:\n",
    "#         if ds in lp_results and isinstance(lp_results[ds], dict) and ds in PAPER_SCORES[\"Linear-Probe\"]:\n",
    "#             local_scores.append(lp_results[ds]['accuracy'])\n",
    "#             paper_scores.append(PAPER_SCORES[\"Linear-Probe\"][ds])\n",
    "#             labels.append(f\"{ds} (Linear-Probe)\")\n",
    "    \n",
    "#     y = np.arange(len(labels))\n",
    "#     width = 0.35\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(12, len(labels) * 0.5))\n",
    "#     ax.barh(y - width/2, paper_scores, width, label=f\"Paper (ViT-B/32)\", color='gray', alpha=0.8)\n",
    "#     ax.barh(y + width/2, local_scores, width, label='My Replication', color='green')\n",
    "    \n",
    "#     ax.set_yticks(y)\n",
    "#     ax.set_yticklabels(labels)\n",
    "#     ax.set_xlabel('Accuracy (%)')\n",
    "#     ax.set_title('Replication Analysis: My Results vs. Paper')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
    "#     ax.set_xlim(0, 100)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"replication_analysis_grouped_library.png\", dpi=300)\n",
    "#     print(\"Plot saved to replication_analysis_grouped_library.png\")\n",
    "#     plt.show()\n",
    "\n",
    "# # --- Plot C: Few-Shot vs. Zero-Shot (Figure 5) ---\n",
    "# def plot_few_shot_analysis(zs_results, fs_results):\n",
    "#     print(\"--- Generating Plot C: Few-Shot Analysis (Replicating Figure 5) ---\")\n",
    "    \n",
    "#     avg_fs_scores = {k: [] for k in K_SHOTS}\n",
    "#     avg_zs_score_list = []\n",
    "    \n",
    "#     datasets = sorted(fs_results.keys())\n",
    "    \n",
    "#     for dataset_name in datasets:\n",
    "#         if dataset_name not in zs_results or not isinstance(zs_results.get(dataset_name), dict):\n",
    "#             print(f\"Skipping {dataset_name}: Missing zero-shot result.\")\n",
    "#             continue\n",
    "        \n",
    "#         fs_data = fs_results.get(dataset_name, {})\n",
    "#         valid_fs_scores = True\n",
    "#         for k in K_SHOTS:\n",
    "#             if not isinstance(fs_data.get(f\"{k}-shot\"), (int, float)):\n",
    "#                 valid_fs_scores = False\n",
    "#                 break\n",
    "        \n",
    "#         if valid_fs_scores:\n",
    "#             print(f\"Including {dataset_name} in average.\")\n",
    "#             avg_zs_score_list.append(zs_results[dataset_name][\"accuracy\"])\n",
    "#             for k in K_SHOTS:\n",
    "#                 avg_fs_scores[k].append(fs_data[f\"{k}-shot\"])\n",
    "#         else:\n",
    "#             print(f\"Skipping {dataset_name}: Missing or failed few-shot results.\")\n",
    "\n",
    "#     if not avg_zs_score_list:\n",
    "#         print(\"Error: No valid common scores found between zero-shot and few-shot results.\")\n",
    "#         return\n",
    "        \n",
    "#     avg_zs = np.mean(avg_zs_score_list)\n",
    "#     avg_fs = {k: np.mean(v) if v else None for k, v in avg_fs_scores.items()}\n",
    "    \n",
    "#     x_fs = [k for k in K_SHOTS if avg_fs[k] is not None]\n",
    "#     y_fs = [avg_fs[k] for k in x_fs]\n",
    "\n",
    "#     print(f\"Average Zero-Shot Accuracy: {avg_zs:.2f}%\")\n",
    "#     for k, v in zip(x_fs, y_fs):\n",
    "#         print(f\"Average {k}-Shot Accuracy: {v:.2f}%\")\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(x_fs, y_fs, marker='o', linestyle='-', label=f\"Average {len(avg_zs_score_list)}-Dataset Linear-Probe\")\n",
    "#     plt.axhline(y=avg_zs, color='r', linestyle='--', label=f\"Average {len(avg_zs_score_list)}-Dataset Zero-Shot ({avg_zs:.2f}%)\")\n",
    "    \n",
    "#     try:\n",
    "#         for i in range(len(x_fs) - 1):\n",
    "#             if y_fs[i] < avg_zs and y_fs[i+1] > avg_zs:\n",
    "#                 x1, y1 = x_fs[i], y_fs[i]\n",
    "#                 x2, y2 = x_fs[i+1], y_fs[i+1]\n",
    "#                 cross_x = x1 + (x2 - x1) * (avg_zs - y1) / (y2 - y1)\n",
    "#                 plt.axvline(x=cross_x, color='gray', linestyle=':', label=f\"Crosses at {cross_x:.1f}-shot\")\n",
    "#                 break\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not calculate intersection: {e}\")\n",
    "\n",
    "#     plt.xlabel('# of Labeled Training Examples per Class (k-shot)')\n",
    "#     plt.ylabel('Average Score (%)')\n",
    "#     plt.title('Few-Shot vs. Zero-Shot Performance')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, linestyle='--', alpha=0.6)\n",
    "#     plt.xticks(K_SHOTS)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"few_shot_analysis_library.png\", dpi=300)\n",
    "#     print(f\"Plot saved to few_shot_analysis_library.png\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ► Run All Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading local results...\")\n",
    "# local_zs_results = load_results(ZERO_SHOT_RESULTS_FILE)\n",
    "# local_lp_results = load_results(LINEAR_PROBE_RESULTS_FILE)\n",
    "# local_fs_results = load_results(FEW_SHOT_RESULTS_FILE)\n",
    "\n",
    "# if not local_zs_results and not local_lp_results and not local_fs_results:\n",
    "#     print(\"\\nError: All results files are missing.\")\n",
    "#     print(\"Please run the 'main_analysis_X' cells above.\")\n",
    "# else:\n",
    "#     if local_zs_results and local_lp_results:\n",
    "#         plot_zeroshot_vs_linear(local_zs_results, local_lp_results)\n",
    "#     else:\n",
    "#         print(\"\\nSkipping Plot A: Missing zero_shot_results.json or linear_probe_results.json.\")\n",
    "        \n",
    "#     plot_replication_analysis(local_zs_results, local_lp_results)\n",
    "    \n",
    "#     if local_zs_results and local_fs_results:\n",
    "#         plot_few_shot_analysis(local_zs_results, local_fs_results)\n",
    "#     else:\n",
    "#         print(\"\\nSkipping Plot C: Missing zero_shot_results.json or few_shot_results.json.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
