{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Baseline Experiments\n",
    "## Zero-Shot, Linear Probe, and Few-Shot Evaluation\n",
    "\n",
    "This notebook demonstrates the complete evaluation pipeline for CLIP baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from models.clip_baseline import CLIPBaseline\n",
    "from utils.config import CLIPConfig\n",
    "from utils.helpers import seed_everything\n",
    "from evaluation.metrics import MetricsTracker\n",
    "from training.trainer_clip import CLIPTrainer\n",
    "from datasets.dataloaders import DatasetFactory\n",
    "from utils.templates import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed_everything(42)\n",
    "\n",
    "# Load configuration\n",
    "config = CLIPConfig(\n",
    "    model_name=\"ViT-B-32\",\n",
    "    pretrained_tag=\"openai\",\n",
    "    batch_size=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Model: {config.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLIP baseline\n",
    "model = CLIPBaseline(config)\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics = MetricsTracker(\"CLIP_BASELINE_NOTEBOOK\", config.results_dir)\n",
    "metrics.track_parameters(model)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CLIPTrainer(model, config, metrics)\n",
    "\n",
    "print(\"\\n✓ Model initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ZERO-SHOT CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load test datasets\n",
    "datasets = DatasetFactory.get_zeroshot_config(model.preprocess, config.data_root)\n",
    "\n",
    "zero_shot_results = {}\n",
    "\n",
    "for dataset_name, dataset_config in datasets.items():\n",
    "    print(f\"\\nEvaluating {dataset_name}...\")\n",
    "    \n",
    "    # Create text classifier\n",
    "    text_classifier = trainer.create_text_classifier(\n",
    "        dataset_config['class_names'],\n",
    "        dataset_config['templates']\n",
    "    )\n",
    "    \n",
    "    # Run zero-shot evaluation\n",
    "    accuracy, predictions, labels = trainer.zero_shot(\n",
    "        dataset_config['dataset'],\n",
    "        text_classifier\n",
    "    )\n",
    "    \n",
    "    zero_shot_results[dataset_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ZERO-SHOT RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for name, result in zero_shot_results.items():\n",
    "    print(f\"{name:20} {result['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Linear Probe Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LINEAR PROBE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load train/test datasets\n",
    "datasets = DatasetFactory.get_linear_probe_datasets(model.preprocess, config.data_root)\n",
    "\n",
    "linear_probe_results = {}\n",
    "\n",
    "for dataset_name, dataset_splits in datasets.items():\n",
    "    print(f\"\\nEvaluating {dataset_name}...\")\n",
    "    \n",
    "    # Run linear probe\n",
    "    accuracy, predictions, labels = trainer.linear_probe(\n",
    "        dataset_splits['train'],\n",
    "        dataset_splits['test']\n",
    "    )\n",
    "    \n",
    "    linear_probe_results[dataset_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINEAR PROBE RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for name, result in linear_probe_results.items():\n",
    "    print(f\"{name:20} {result['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEW-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select one dataset for few-shot analysis\n",
    "dataset_name = \"CIFAR100\"\n",
    "dataset_splits = datasets[dataset_name]\n",
    "\n",
    "print(f\"\\nEvaluating {dataset_name} with k-shot learning...\")\n",
    "\n",
    "# Run few-shot evaluation\n",
    "few_shot_results, test_labels = trainer.few_shot(\n",
    "    dataset_splits['train'],\n",
    "    dataset_splits['test'],\n",
    "    k_shots=[1, 2, 4, 8, 16, 32]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEW-SHOT RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for k_shot, result in few_shot_results.items():\n",
    "    print(f\"{k_shot:10} {result['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Zero-Shot vs Linear Probe comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "dataset_names = list(zero_shot_results.keys())\n",
    "zs_accuracies = [zero_shot_results[name]['accuracy'] for name in dataset_names]\n",
    "lp_accuracies = [linear_probe_results[name]['accuracy'] for name in dataset_names]\n",
    "\n",
    "x = np.arange(len(dataset_names))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, zs_accuracies, width, label='Zero-Shot', alpha=0.8)\n",
    "ax.bar(x + width/2, lp_accuracies, width, label='Linear Probe', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Zero-Shot vs Linear Probe Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(dataset_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/clip_baseline_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Few-Shot Learning Curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "k_values = [int(k.split('-')[0]) for k in few_shot_results.keys()]\n",
    "accuracies = [few_shot_results[k]['accuracy'] for k in few_shot_results.keys()]\n",
    "\n",
    "ax.plot(k_values, accuracies, marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of shots per class (k)')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title(f'Few-Shot Learning Curve - {dataset_name}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/clip_baseline_fewshot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track final metrics\n",
    "metrics.track_performance(\n",
    "    accuracy=np.mean([r['accuracy'] for r in zero_shot_results.values()]),\n",
    "    loss=0.0\n",
    ")\n",
    "\n",
    "# Save all metrics\n",
    "metrics.save_metrics(run_id=\"notebook_experiment\")\n",
    "metrics.print_summary()\n",
    "\n",
    "print(\"\\n✓ Results saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
